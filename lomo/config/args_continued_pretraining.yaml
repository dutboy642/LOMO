# Configuration for Continued Pretraining with LOMO

# Model
model_name_or_path: 'meta-llama/Meta-Llama-3-8B'  # Base model to continue training
cache_dir: '/mnt/lomo'
local_file: false

# Dataset configuration (NEW: Configure dataset via YAML)
dataset_name: 'wikipedia_ja_100_samples'  # Dataset name for logging/tracking
dataset_path: 'data/wikipedia_ja_100_samples.json'  # Path to dataset file or HF repo (e.g., 'username/repo-name')
text_column: 'text'  # Column name containing text data
sample_size: 100  # Number of samples to use (-1 for all data)

# Data processing
refresh: false  # Set to true to reprocess data (set to false after first run)
data_tag: 'continued_pretraining'
train_on_inputs: false  # For continued pretraining, always true (train on entire text)
data_max_length: 512  # Sequence length - adjust based on your GPU memory
data_dir: 'data'  # Directory where processed data will be stored

# Training
tag: 'lomo_continued_pretraining'
output_dir: '/mnt/lomo/outputs'
overwrite_output_dir: true
deepspeed: 'config/ds_config.json'
do_train: true
do_eval: false # set false as eval is not common in continued pretraining
# evaluation_strategy: 'steps'  # Changed to 'steps' for more frequent eval
# eval_steps: 500  # Evaluate every 500 steps
logging_steps: 100  # Log every 100 steps

# Batch size settings (adjust based on your GPU memory)
per_device_train_batch_size: 1  # Smaller batch size for longer sequences
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8  # Effective batch size = 4 * 4 = 16

# Learning rate (continued pretraining typically uses lower lr than fine-tuning)
learning_rate: 1e-5  # Much lower than fine-tuning (usually 1e-5 to 5e-5)
weight_decay: 0.01  # Small weight decay for regularization
num_train_epochs: 3  # Fewer epochs for continued pretraining
lr_scheduler_type: 'linear'  # only linear and constant schedulers are supported
warmup_steps: 100  # Warmup steps instead of ratio
adam_beta1: 0.9
adam_beta2: 0.95  # Common for continued pretraining
adam_epsilon: 1e-8

# Gradient clipping
clip_grad_norm: 1.0

# Wandb configuration
wandb_mode: 'online'  # Options: 'online', 'offline', 'disabled'
wandb_project: 'lomo-continued-pretraining'
wandb_entity: null  # Change this to your wandb entity/username
# wandb_run_name: null  # Optional: custom run name, auto-generated if null

# Checkpointing
save_strategy: 'epoch'
# save_steps: 1000  # Save checkpoint every 1000 steps
save_total_limit: 3  # Keep only 3 most recent checkpoints
load_best_model_at_end: false  # Not applicable for continued pretraining

# Other settings
seed: 42
remove_unused_columns: false
dataloader_pin_memory: false
gradient_checkpointing: true  # Essential for memory efficiency
predict_with_generate: false
group_by_length: true  # Group sequences by length for efficiency

# Logging
# DON'T set report_to - it's handled automatically
# report_to: 'wandb'  # Use wandb for experiment tracking
logging_dir: 'logs'
logging_first_step: true

# Mixed precision
bf16: false  # Use fp16 instead of bf16 if your GPU doesn't support bf16
fp16: true
dataloader_num_workers: 0

# Resume from checkpoint (uncomment to resume training)
# resume_from_checkpoint: 'outputs/wikipedia_ja_llama-7b_lomo_continued_pretraining/continued_pretraining_lr1e-05_bs4_cosine_warmup100_clipnorm1.0/checkpoint-1000'